{% extends "base-container.html" %}

{% block title %}quick-and-dirty-jupyter-notebook-pipelines{% endblock %}
{% block head %}
{{ super() }}
{{ prism_highlighting.css | safe }}
<link href="{{ url_for('quick_and_dirty_jupyter_notebook_pipelines.static', filename='style.css') }}" rel="stylesheet">
{% endblock %}

{% block container %}
{% filter markdown %}

# quick and dirty jupyter notebook pipelines

Jupyter notebooks are web pages connected to a programming language. They are extremely handy for ad hoc. They are on
of the easiest ways to perform and
share exploratory analysis, ..., and they can even be used as "dashboards" for batch processing â€“ and this is where
coordinating the execution of notebooks into a data "pipeline". Here's how to do this. Places like Netflix has
committed to Jupyter notebooks being first class citizens, including the ability to coordinate the execution of
notebooks into staged pipelines.

Quick-and-dirty notebook pipelines are a way to monitor the entire process from collection to model to testing. You get
a lot visibility with very little complexity.

Dev setup is...

- Easy to setup.
- Easy to execute.
- Easy to debug.
- Easy to deploy.

Such that we get...

We're going to have a series of web pages dynamically generated, these web pages are actual CI/CD

- Dynamically generated graphs.
- Process metadata.
- Logging.

That easy...

- Easy to inspect.
- Easy to share.

Caveats:

- We assume the data, model is small enough to run everything locally.
- With that said, there are many opportunities to build off of this.

## the sitch

A colleague provides access to some data. You research it, clean it, and train a predictive
model using its features. The results are useful, you want to continually update the model with new data as a batch
job. How are you going to keep an eye on how things are going?

Let's take what you've already done and turn it into a notebook pipeline. With just a few steps, you'll be able to
download your code, install the package, execute the pipeline, and get the updated notebooks and other metadata as
output to monitor each batch.

## development setup

First thing is setting up our development environment. Here's how we setup the directory structure.

<pre><code class="language-bash">
package/
  requirements.txt     # Our dependencies.
  setup.py             # Helps install the package.
  bin/                 # Bash scripts for executing pipeline.
    train.sh
    test.sh
    run.sh
  package/             # Capture routines here, keep notebooks concise.
    __main__.py        # The CLI.
    module_a/          # Modules are nice.
    ...
  notebook-pipeline/   # Exactly what it says.
    notebook-1.ipynb   # First notebook in the pipeline.
    ...                # ...
    notebook-1.ipynb   # Last notebook in the pipeline
</code></pre>

From here we create a virtual environment, install dependencies and package, configure a kernel for our notebooks to
use.

`pip install -e .` and `cond add kernel?`

## bash script

Sequencing the notebooks is as easy as a bash script and `nbconvert`.

<pre><code class="language-c">
# Provide an output target directory argument.
# ./run.sh
OUTDIR=$1                                # First CLI argument.
mkdir -p $OUTDIR                         # Create the output target.
# Run the pipeline notebook.
# Execute each notebook in sequence.
for filename in [0-9]*ipynb; do          # Iterate through notebooks in order.
  jupyter nbconvert --to html \          # Convert them to HTML.
    --output-dir=$OUTDIR \               # Write them here.
    --ExecutePreprocessor.timeout=600 \  # Make sure we give them some time to run.
    $filename \                          # The current notebook.
    --execute                            # Execute them before converting to HTML
done                                     # Fin.
</code></pre>

## deploying

### Locally

- Clone repo.
- Create environment.
- Install.
- Run.
- Automate batch via cron job.

### Remotely

- AWS?
- `buildspec.yml`
- GitHub build trigger.

<pre><code class="language-c">

</code></pre>


## example

TODO GitHub example?

{% endfilter %}
{% endblock %}

{% block footer %}
{{ super() }}
{{ prism_highlighting.js | safe }}
<script src="{{ url_for('quick_and_dirty_jupyter_notebook_pipelines.static', filename='quick-and-dirty-jupyter-notebook-pipelines.js') }}"
        type="text/javascript"></script>
<script>
    $("#loading").removeClass('active');
</script>
{% endblock %}
